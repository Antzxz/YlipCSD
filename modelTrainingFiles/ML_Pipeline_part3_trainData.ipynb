{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Code was built for the initial model training.\n",
    "ML Pipeline Code was built for the initial model training detailed in \"Integrated knowledge mining, genome-scale modeling, and machine learning for predicting *Yarrowia lipolytica* bioproduction\".\n",
    "\n",
    "### Part 3/4:\n",
    "* Part 1: Performs data importation, intial formatting and splits data into 3 parts for training, validation, and testing.\n",
    "* Part 2: FBA feature generation is completed; script entitled \"ML_pipeline_JC_part2\"\n",
    "* Part 3: Feature encoding is completed; script entitled \"ML_pipeline_part3\"\n",
    "* Part 4: Machine learning model training is completed; script entitled \"ML_pipeline_part4\"\n",
    "    \n",
    "### Inputs:\n",
    "* pickle file: Train&ValidateData_part2.pickle or TESTData_part2.pickle from Part 2 are inputs to the script. \n",
    "* Data encoding file: Publication entitled file: 'Supplemental Excel File 2- Supplemental Excel File 3- DataCharateristics & Encoding.xlsx'\n",
    "\n",
    "### Outputs:    \n",
    "\n",
    "* A pickle datafile entitled \"Train&ValidateData_part3.pickle\" or \"TESTData_part3.pickle\" at the end of the file.\n",
    "    \n",
    "#### Additional Info: This file can also be used for non-Yarrowia lipoytica GSMs and has been validated with *Rhodosporidium toruloides* and *Cutaneotrichosporon oleaginosus*.\n",
    "\n",
    "### Note:\n",
    "This file process the train&validate and TEST data as separate branches. It must be ran twice in order to get the full data, but changing the input file from Part 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell imports the necessary libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipidSimplificationOption=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell contains functions for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell takes the database input and outputs different encoding for each class\n",
    "\n",
    "#categorically encodes the volume to 1-5 (smallest to largest) based on cultivation volume.\n",
    "def rxtVolumeEncoding(x):\n",
    "    if x <= 0.01:\n",
    "        return 1\n",
    "    elif x <= 0.075:\n",
    "        return 2\n",
    "    elif x <= 0.25:\n",
    "        return 3\n",
    "    elif x < 1:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "#categorically encodes the reactor vessel type (micro-reactors, shaking flasks, batch fedbatch or continuous vessels).\n",
    "#smallest (1) to largest (3).\n",
    "def reactorTypeEncoding(x):\n",
    "    if x == 2:\n",
    "        return 3\n",
    "    elif x == 4:\n",
    "        return 3\n",
    "    elif x==3:\n",
    "        return 3\n",
    "    elif x == 1:\n",
    "        return 2\n",
    "    elif x == 5:\n",
    "        return 1\n",
    "\n",
    "#corrects the database encoding to categorically encode from lowest oxygen level (1) to highest (3).    \n",
    "def oxygenEncodingFix(x):\n",
    "    if x==1:\n",
    "        return 3 #3 not oxygen sufficient\n",
    "    elif x==2:\n",
    "        return 1 #1 now oxygen insufficeint\n",
    "    elif x==3:\n",
    "        return 2 #2 now intermidiate\n",
    "\n",
    "#corrects the database encoding to categorically encode from lowest nitrogen level (1) to highest (3).        \n",
    "def nitrogenEncodingFix(x):\n",
    "    if x==1:\n",
    "        return 3 #nitrogen sufficent\n",
    "    elif x==2:\n",
    "        return 1 #nitrogen limited\n",
    "    elif x==3:\n",
    "        return 2 #intermeidiate\n",
    "\n",
    "def funcReturnStr(x):\n",
    "    return str(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the DataStructure file is in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict for enocoding.\n",
    "\n",
    "encoding_Data=pd.ExcelFile('Supplemental Excel File 2- DataCharateristics & Encoding.xlsx').parse('Encoding')\n",
    "\n",
    "\n",
    "#not all encoding dict features were used in the final model. \n",
    "strainDict = dict(zip(encoding_Data.strain_background,encoding_Data.strain_class))\n",
    "mediaDict = dict(zip(encoding_Data.media,encoding_Data.media_class))\n",
    "productDict = dict(zip(encoding_Data.Product, encoding_Data.prdt_class))\n",
    "carbonSourceMWDict = dict(zip(encoding_Data.carbonSource,encoding_Data.carbonSourceMW))\n",
    "N2sourceDict = dict(zip(encoding_Data.N2Source,encoding_Data.N2source_class))\n",
    "promoterDict = dict(zip(encoding_Data.Promoters,encoding_Data.prom_class))\n",
    "integrationSiteDict = dict(zip(encoding_Data.integrationSite,encoding_Data.int_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the pickle file from Part 2 is in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the data from part 2.\n",
    "FBATrainData = pd.DataFrame()\n",
    "\n",
    "# # TEST data.\n",
    "\n",
    "# with open('TESTData_part2.pickle','rb') as f:   \n",
    "#     Data = pickle.load(f)\n",
    "\n",
    "# # Train & validate data.\n",
    "with open('Train&ValidateData_part2.pickle','rb') as f:   \n",
    "    Data = pickle.load(f)\n",
    "\n",
    "FBATrainData = Data[0]\n",
    "workingData = FBATrainData.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingData['product_name2'] = workingData['product_name']\n",
    "workingData['product_name'] = workingData.product_name.map(productDict).fillna(workingData.product_name)\n",
    "workingData['strain_background'] = workingData.strain_background.map(strainDict).fillna(workingData.strain_background)\n",
    "workingData['media'] = workingData.media.map(mediaDict).fillna(workingData.media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create encoded feature for Carbon Source quality and quantity based off of Delta G' and concentration added to culture.\n",
    "\n",
    "workingData['carbonSourceOneMolecularWeight'] = workingData.cs1.map(carbonSourceMWDict).fillna(workingData.cs1)\n",
    "workingData['carbonSourceTwoMolecularWeight'] = workingData.cs2.map(carbonSourceMWDict).fillna(workingData.cs2)\n",
    "\n",
    "temp1 = workingData.cs_conc1/workingData.carbonSourceOneMolecularWeight*workingData['cs1_heatCombustion(kJ/mol)']\n",
    "temp2 = workingData.cs_conc2/workingData.carbonSourceTwoMolecularWeight*workingData['cs2_heatCombustion(kJ/mol)']\n",
    "\n",
    "temp2.fillna(0,inplace=True)\n",
    "df = pd.DataFrame()\n",
    "df['one'] = temp1\n",
    "df['two'] = temp2\n",
    "\n",
    "temp3={}\n",
    "\n",
    "for y,z in enumerate(df.one):\n",
    "    if df.two.iloc[y]!=0:\n",
    "        temp3[df.index[y]]=df.one.iloc[y]+df.two.iloc[y]\n",
    "    else:\n",
    "        temp3[df.index[y]]=df.one.iloc[y]\n",
    "\n",
    "temp3 = pd.Series(temp3)\n",
    "\n",
    "workingData['inputThermo(kJ/L)'] = temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precursors required\n",
    "temp2 = pd.DataFrame()\n",
    "temp2 = workingData.precursor_required.apply(funcReturnStr).str.split(';',expand=True).fillna(0) #TAG\n",
    "temp2 = temp2.apply(pd.to_numeric)\n",
    "workingData['precursorsRequiredEncoded'] = temp2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a thermodynamic barrier features (total thermodynamic barrier from precursor to product and average).\n",
    "totalTherm={}\n",
    "averageTherm={}\n",
    "\n",
    "\n",
    "for dataPoint in workingData.index:\n",
    "    \n",
    "    stoichNADPH=(workingData.nadh_nadph_cost.loc[dataPoint])\n",
    "    stoichATP=(workingData.atp_cost.loc[dataPoint])\n",
    "    stoichprecursor={}\n",
    "    temp1={}\n",
    "    temp2={}\n",
    "    # prec=[]\n",
    "\n",
    "    #Thermodynamics used the fatty acid based information instead of the TAG molecules.\n",
    "    if workingData.loc[dataPoint]['product_name2']=='Lipids' and lipidSimplificationOption==1:\n",
    "        stoichATP = stoichATP/3\n",
    "        stoichNADPH = stoichNADPH/3\n",
    "        temp2=workingData.loc[dataPoint].precursor_required.strip().split(';')\n",
    "        stoichprecursor[0] = float(temp2[0])/3\n",
    "\n",
    "        temp1[0] = -3341.2 #deltaGo\n",
    "        prec = ['Acetyl-CoA']\n",
    "        \n",
    "    # non-lipid compounds\n",
    "    else:\n",
    "        prec = workingData.loc[dataPoint].central_carbon_precursor.strip().split(';')\n",
    "\n",
    "        if isinstance(workingData.loc[dataPoint].precursor_required,str):\n",
    "            stoichprecursor=workingData.loc[dataPoint].precursor_required.strip().split(';')\n",
    "            temp1 = workingData.loc[dataPoint]['ccm_precursor_deltaGo'].strip().split(';')\n",
    "        else:\n",
    "            stoichprecursor[0]=workingData.loc[dataPoint].precursor_required\n",
    "            temp1[0] = workingData.loc[dataPoint]['ccm_precursor_deltaGo']\n",
    "\n",
    "    thermoTemp = 0\n",
    "    for i,j in enumerate(prec):\n",
    "\n",
    "        thermoTemp += float(stoichprecursor[i])*float(temp1[i])\n",
    "        if j == 'Acetyl-CoA':\n",
    "            stoichCoA = float(stoichprecursor[i])\n",
    "        else:\n",
    "            stoichCoA = 0\n",
    "\n",
    "    ATP_nadph_tempThermo = stoichATP*-31.8+-28.8*stoichNADPH #deltaGo\n",
    "\n",
    "    totalTherm[dataPoint] = round(ATP_nadph_tempThermo + workingData.loc[dataPoint]['product_deltaGo']-thermoTemp+stoichCoA*-3202.2)\n",
    "\n",
    "    if (totalTherm[dataPoint]==0 and workingData.loc[dataPoint]['Pathway_enzymatic_steps']==0):\n",
    "        averageTherm[dataPoint]=0\n",
    "    else:\n",
    "        averageTherm[dataPoint]=round(totalTherm[dataPoint]/(workingData.loc[dataPoint]['Pathway_enzymatic_steps']))\n",
    "\n",
    "workingData['totalThermBarrier'] = totalTherm\n",
    "workingData['averageThermBarrier'] = averageTherm\n",
    "workingData['averageThermBarrier'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N2 Source Encoding\n",
    "temp1 = pd.DataFrame()\n",
    "\n",
    "temp1 = workingData.N2Source.apply(funcReturnStr).str.split(';',expand=True).fillna('NaN') #FA\n",
    "for col in temp1.columns:\n",
    "    temp1[col]=temp1[col].map(N2sourceDict)\n",
    "\n",
    "workingData['N2SourceEncoded(mean)'] = temp1.mean(axis=1).fillna(0.5)\n",
    "workingData['N2SourceEncoded(max)'] = temp1.max(axis=1).fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "# Options: High N2 and organic source; high N2 and inorganic source; low N2 and organic source; low N2 and inoranic source. \n",
    "temp1={}\n",
    "for dataPoint in workingData.index:\n",
    "    if workingData.loc[dataPoint]['N2SourceEncoded(max)']==1 and workingData.loc[dataPoint]['N2_content']>1:\n",
    "        temp1[dataPoint]=4 # high N2, organic\n",
    "    elif workingData.loc[dataPoint]['N2SourceEncoded(max)']==1 and workingData.loc[dataPoint]['N2_content']<2:\n",
    "        temp1[dataPoint]=2 #low N2, organic\n",
    "    elif workingData.loc[dataPoint]['N2SourceEncoded(max)']==0 and workingData.loc[dataPoint]['N2_content']<2:\n",
    "        temp1[dataPoint]=1 #low N2, inorganic\n",
    "    elif workingData.loc[dataPoint]['N2SourceEncoded(max)']==0 and workingData.loc[dataPoint]['N2_content']>1:\n",
    "        temp1[dataPoint]=3 #high N2, inorganic\n",
    "\n",
    "\n",
    "temp1 = pd.Series(temp1)\n",
    "workingData['N2_contentEncoded']=temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration site & Promoter strength Encoding\n",
    "## Not utilized in final model.\n",
    "\n",
    "temp1 = pd.DataFrame()\n",
    "temp2 = pd.DataFrame()\n",
    "\n",
    "temp1 = workingData.integration_site_Filled.apply(funcReturnStr).str.split(';',expand=True).fillna('NaN')\n",
    "for col in temp1:\n",
    "    temp1[col]=temp1[col].map(integrationSiteDict)\n",
    "\n",
    "\n",
    "workingData['integrationSiteEncoded(Sum)'] = temp1.sum(axis=1).fillna(0)\n",
    "workingData['integrationSiteEncoded(Mean)'] = temp1.mean(axis=1).fillna(0)\n",
    "\n",
    "\n",
    "temp2 = workingData.gene_promoter.apply(funcReturnStr).str.split(';',expand=True).fillna('NaN')\n",
    "\n",
    "for col in temp2:\n",
    "    temp2[col]=temp2[col].map(promoterDict)\n",
    "\n",
    "workingData['promoterEncoded(Sum)'] = temp2.sum(axis=1).fillna(0)\n",
    "workingData['promoterEncoded(Mean)'] = temp2.mean(axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data encoding.\n",
    "workingData.rxt_volume = workingData.rxt_volume.apply(rxtVolumeEncoding)\n",
    "workingData.reactor_type = workingData.reactor_type.apply(reactorTypeEncoding)\n",
    "workingData.oxygen = workingData.oxygen.apply(oxygenEncodingFix)\n",
    "workingData.pH = workingData.pH.fillna(6.8)\n",
    "workingData['csConcTotal'] = workingData['cs_conc1']+ workingData['cs_conc2']\n",
    "workingData['dir_evo'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBA features encoding. \n",
    "\n",
    "# workingData['O2Uptake_iYL_2.0'] = abs(workingData['O2Uptake_iYL_2.0'])\n",
    "# workingData['O2Uptake_iNL895'] = abs(workingData['O2Uptake_iNL895'])\n",
    "workingData['O2Uptake_iYLI647'] = abs(workingData['O2Uptake_iYLI647'])\n",
    "# workingData['O2Uptake_iMK735'] = abs(workingData['O2Uptake_iMK735'])\n",
    "# workingData['O2Uptake_iYali4'] = abs(workingData['O2Uptake_iYali4'])\n",
    "# workingData['O2Uptake_Coleaginosus'] = abs(workingData['O2Uptake_Coleaginosus'])\n",
    "# workingData['O2Uptake_iRhtoC'] = abs(workingData['O2Uptake_iRhtoC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "with open('Train&ValidateData_part3.pickle', 'wb') as f:\n",
    "    pickle.dump([workingData], f)\n",
    "    \n",
    "# with open('Train&ValidateData_part3.pickle', 'wb') as f:\n",
    "#     pickle.dump([workingData], f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaPY36lin",
   "language": "python",
   "name": "condapy36lin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
