{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Code was built for the initial model training.\n",
    "ML Pipeline Code was built for the initial model training detailed in \"Integrated knowledge mining, genome-scale modeling, and machine learning for predicting *Yarrowia lipolytica* bioproduction\".\n",
    "\n",
    "### Part 1/4:\n",
    "* Part 1: Performs data importation, intial formatting and splits data into 3 parts for training, validation, and testing.\n",
    "* Part 2: FBA feature generation is completed; script entitled \"ML_pipeline_part2\"\n",
    "* Part 3: Feature encoding is completed; script entitled \"ML_pipeline_part3\"\n",
    "* Part 4: Machine learning model training is completed; script entitled \"ML_pipeline_part4\"\n",
    "    \n",
    "### Inputs:\n",
    "* Database file: Machine learning. Publication entitled file: 'Supplemental Excel File 1- Database.xlsx'\n",
    "* Data encoding file: Publication entitled file: 'Supplemental Excel File 2- DataCharateristics & Encoding.xlsx'\n",
    "\n",
    "### Outputs:    \n",
    "* A pickle datafile entitled \"Train&ValidateData_part1.pickle\" & \"TESTData_part1.pickle\" is created at the end of the file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell initializes the packages needed for the data importing.\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure you are in folder with database & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impport the data from the database file.\n",
    "\n",
    "#find the file\n",
    "dir_path = os.path.dirname(os.path.relpath('Supplemental Excel File 1- Database.xlsx'))\n",
    "file_path = os.path.join(dir_path,'Supplemental Excel File 1- Database.xlsx')\n",
    "\n",
    "raw_data = pd.read_excel(file_path,sheet_name='data',skiprows=range(1))\n",
    "\n",
    "# raw_data=pd.ExcelFile(file_path).parse('data')\n",
    "raw_data_col_headers = raw_data.columns\n",
    "\n",
    "\n",
    "# specifies that the zero product production instances will be dropped.\n",
    "drop0option=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all the information in the database was used for training. The unnecessary columns are removed from the datalist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3070\n"
     ]
    }
   ],
   "source": [
    "# several columns in the original database were not used for training due to incomplete information. This cell cleans up the data.\n",
    "so_far_not_filled_in_cols = ['Laststep_energy_barrier_step','Relative_promoter_strength','sensor_regulator','Mod_path_opt','engineering_approaches']\n",
    "\n",
    "alternatives_toTest = ['Pathway_enzymatic_steps','atp_cost','nadh_nadph_cost']\n",
    "\n",
    "so_far_useless = ['TF','integration_site_plasmids','blank1','blank2','blank3','blank4','blank5','blank6','blank7','temp_holding']\n",
    "\n",
    "columnsToDelete = ['cs1_equilibrator_deltaG0','cs1_deltaG\\'0','cs2_equilibrator_deltaG0','cs2_deltaG\\'0','Product_yield(g/DCW)','Biomass_titer(g/L)','Biomass_growth_rate']\n",
    "\n",
    "\n",
    "# Make a list of the columns that are used.\n",
    "useful_cols = list(set(raw_data_col_headers)-set(so_far_not_filled_in_cols)-set(so_far_useless)-set(columnsToDelete))\n",
    "\n",
    "#\n",
    "data = raw_data.loc[:, useful_cols]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Engineered gene feature categories from the input data.\n",
    "\n",
    "data['number_genes_mod'] = data.genes_modified_updated.apply(lambda x: x.count(';')+1 if isinstance(x,str) else 0)\n",
    "# data['number_genes_mod']\n",
    "\n",
    "data['number_genes_deleted'] = data.gene_deletion.apply(lambda x: x.count('1') if isinstance(x,str) else 0)\n",
    "# data['number_genes_deleted']\n",
    "\n",
    "data['number_total_genes_overexp'] = data.gene_overexpression.apply(lambda x: x.count('1') if isinstance(x,str) else 0)\n",
    "# data['number_total_genes_overexp']\n",
    "\n",
    "data['number_genes_het'] = data.heterologous_gene.apply(lambda x: x.count('1') if isinstance(x,str) else 0)\n",
    "# data['number_genes_het']\n",
    "\n",
    "#number of heterlogous genes\n",
    "hettemp1 = data.heterologous_gene#.apply(lambda x: x if isinstance(x,str) else 'NA')\n",
    "hettemp2 = hettemp1.str.split(';',expand=True)\n",
    "overexpressTemp1 = data.gene_overexpression.fillna('2')\n",
    "overexpressTemp2 = overexpressTemp1.str.split(';',expand=True)\n",
    "nativeGenes = overexpressTemp2[hettemp2=='0']\n",
    "\n",
    "data['number_native_genes_overexp'] = nativeGenes.count(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the DataStructure file is within the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915\n"
     ]
    }
   ],
   "source": [
    "#Load data encoding information contained in the DataStructure sheet.\n",
    "encoding_Data=pd.ExcelFile('Supplemental Excel File 2- DataCharateristics & Encoding.xlsx').parse('Encoding')\n",
    "\n",
    "#product super classes (1-9 general classes, such as Lipids, small terpenes, flavenoids, etc.).\n",
    "productDict = dict(zip(encoding_Data.Product, encoding_Data.prdt_class))\n",
    "data['product_name2'] = data.product_name.map(productDict).fillna(data.product_name) #save original product name.\n",
    "\n",
    "#drop the zero product instances.\n",
    "if drop0option==1:\n",
    "    data = (data.loc[data['Product_titer(g/L)']!=0])\n",
    "    data['Product_titer(g/L)'].dropna(inplace=True,axis='rows')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & validating instances: 2257\n",
      "Testing instances: 658 \n",
      "\n",
      "Training instances of product class 1 : 673\n",
      "Training instances of product class 2 : 370\n",
      "Training instances of product class 3 : 157\n",
      "Training instances of product class 4 : 651\n",
      "Training instances of product class 5 : 47\n",
      "Training instances of product class 6 : 86\n",
      "Training instances of product class 7 : 147\n",
      "Training instances of product class 8 : 18\n",
      "Training instances of product class 9 : 108\n",
      "\n",
      "\n",
      "Testing instances of product class 1 : 192\n",
      "Testing instances of product class 2 : 102\n",
      "Testing instances of product class 3 : 77\n",
      "Testing instances of product class 4 : 175\n",
      "Testing instances of product class 5 : 12\n",
      "Testing instances of product class 6 : 32\n",
      "Testing instances of product class 7 : 37\n",
      "Testing instances of product class 8 : 4\n",
      "Testing instances of product class 9 : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjczajka/miniconda3/envs/condaPY36lin/lib/python3.6/site-packages/pandas/core/frame.py:4170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "#Create the training split for the data. \n",
    "Train, Test = train_test_split(data, test_size = 0.2, random_state = 56,stratify = data.product_name2)\n",
    "    \n",
    "# add specific products to the test set, remove fromt the training set.\n",
    "# productsToTestCases = ['Arachidonic acid', 'a-Ionone', 'Valencene', 'Mevalonate', 'Campersterol', 'Crotonic acid', 'a-Farnesene', 'Riboflavin', '1-decanol']\n",
    "productsToTestCases = ['a-Ionone', 'Mevalonate', 'Campersterol', 'a-Farnesene', '1-decanol','Arachidonic acid']\n",
    "\n",
    "for p in productsToTestCases:\n",
    "    temp = Train[Train.product_name == p]\n",
    "    tempIndex = Train[Train.product_name ==p].index\n",
    "    Train.drop(tempIndex,inplace=True)\n",
    "    Test = Test.append(temp)\n",
    "\n",
    "print('Training & validating instances:',len(Train))\n",
    "print('Testing instances:',len(Test),'\\n')\n",
    "    \n",
    "for p in productsToTestCases:\n",
    "    t = Test[Test.product_name == p]\n",
    "    tr = Train[Train.product_name == p]\n",
    "#     print(p,'test cases: ',len(t), 'training cases: ',len(tr))\n",
    "\n",
    "data['product_name2'] = data.product_name.map(productDict).fillna(data.product_name) #save original product name.\n",
    "\n",
    "for i in range(1,10):\n",
    "    print('Training instances of product class',i,':',(Train.product_name2==i).sum())\n",
    "    \n",
    "print('\\n')\n",
    "for i in range(1,10):    \n",
    "    print('Testing instances of product class',i,':',(Test.product_name2==i).sum())\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pickle files containing data.    \n",
    "with open('TESTData_part1.pickle', 'wb') as f:\n",
    "    pickle.dump([Test], f)\n",
    "with open('Train&ValidateData_part1.pickle', 'wb') as f:\n",
    "    pickle.dump([Train], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaPY36lin",
   "language": "python",
   "name": "condapy36lin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
